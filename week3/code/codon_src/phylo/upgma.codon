# This source code is part of the Biotite package and is distributed
# under the 3-Clause BSD License. Please see 'LICENSE.rst' for further
# information.

__name__ = "biotite.sequence.phylo"
__author__ = "Patrick Kunzmann"
__all__ = ["upgma"]


# import numpy as np

from .tree import Tree, Node
import numpy as np

# ctypedef np.float64_t float64
# ctypedef np.uint8_t uint8
# ctypedef np.uint32_t uint32 
# I have never heard of using typedef in python


MAX_FLOAT = np.finfo(np.float64).max


# @cython.boundscheck(False) Per Cython for Numpy users these introduce overhead but mean you won't get index out of bounds when set to True.
# @cython.wraparound(False) I haven't seen anything about this in codon so I'm just commenting it out.
def upgma( distances: np.ndarray):
    """
    upgma(distances)
    
    Perform hierarchical clustering using the
    *unweighted pair group method with arithmetic mean* (UPGMA).
    
    This algorithm produces leaf nodes with the same distance to the
    root node.
    In the context of evolution this means a constant evolution rate
    (molecular clock).

    Parameters
    ----------
    distances : ndarray, shape=(n,n)
        Pairwise distance matrix.

    Returns
    -------
    tree : Tree
        A rooted binary tree. The `index` attribute in the leaf
        :class:`Node` objects refer to the indices of `distances`.

    Raises
    ------
    ValueError
        If the distance matrix is not symmetric
        or if any matrix entry is below 0.

    Examples
    --------
    
    >>> distances = np.array([
    ...     [0, 1, 7, 7, 9],
    ...     [1, 0, 7, 6, 8],
    ...     [7, 7, 0, 2, 4],
    ...     [7, 6, 2, 0, 3],
    ...     [9, 8, 4, 3, 0],
    ... ])
    >>> tree = upgma(distances)
    >>> print(tree.to_newick(include_distance=False))
    ((4,(3,2)),(1,0));
    """
    i:int = 0
    j:int = 0
    k:int = 0

    i_min:int = 0
    j_min:int = 0
    0
    dist:float 
    dist_min:float
    mean:float
    height:float

    if distances.shape[0] != distances.shape[1] \
        or not np.allclose(distances.T, distances):
            raise ValueError("Distance matrix must be symmetric")
    if np.isnan(distances).any():
        raise ValueError("Distance matrix contains NaN values")
    if (distances >= MAX_FLOAT).any():
        raise ValueError("Distance matrix contains infinity")
    if (distances < 0).any():
        raise ValueError("Distances must be positive")


    # Keep track on clustered indices
    nodes:np.ndarray = np.array(
        [Node(index=i) for i in range(distances.shape[0])]
    )
    # Indicates whether an index in the distance matrix has already been
    # clustered and the repsective rows and columns can be ignored
    is_clustered_v:np.ndarray = np.full( # Unsigned 8 bit integer originally
        distances.shape[0], False, dtype=np.uint8
    )
    # Number of indices in the current node (cardinality)
    # (required for proportional averaging)
    cluster_size_v:np.ndarray = np.ones( # Unsigned 32 bit integer originally
        distances.shape[0], dtype=np.float64
    )
    # Distance of each node from leaf nodes,
    # used for calculation of distance to child nodes
    node_heights: np.ndarray = np.zeros(
        distances.shape[0], dtype=np.float64
    )


    # Cluster indices
    # cdef float64[:,:] distances_v = distances.astype(np.float64, copy=True)
    distances_v:np.ndarray = distances.astype(np.float64, copy=True)

    # Exit loop via 'break'
    while True:

        # Find minimum distance
        dist_min = MAX_FLOAT
        i_min = -1
        j_min = -1
        for i in range(distances_v.shape[0]):
            if is_clustered_v[i]:
                continue
            for j in range(i):
                if is_clustered_v[j]:
                    continue
                dist = distances_v[i,j]
                if dist < dist_min:
                    dist_min = dist
                    i_min = i
                    j_min = j
        
        if i_min == -1 or j_min == -1:
            # No distance found -> all leaf nodes are clustered
            # -> exit loop
            break
        
        # Cluster the nodes with minimum distance
        # replacing the node at position i_min
        # leaving the node at position j_min empty
        # (is_clustered_v -> True)
        height = dist_min/2
        nodes[i_min] = Node(
            [nodes[i_min], nodes[j_min]],
            [height-node_heights[i_min], height-node_heights[j_min]]
        )
        node_heights[i_min] = height
        # Mark position j_min as clustered
        nodes[j_min] = None
        is_clustered_v[j_min] = True
        # Calculate arithmetic mean distances of child nodes
        # as distances for new node and update matrix
        for k in range(distances_v.shape[0]):
            if not is_clustered_v[k] and k != i_min:
                mean = (
                    (
                          distances_v[i_min,k] * cluster_size_v[i_min]
                        + distances_v[j_min,k] * cluster_size_v[j_min]
                    ) / (cluster_size_v[i_min] + cluster_size_v[j_min])
                )
                distances_v[i_min,k] = mean
                distances_v[k,i_min] = mean
        # Updating cluster size of new node
        cluster_size_v[i_min] = cluster_size_v[i_min] + cluster_size_v[j_min]
    

    # As each higher level node is always created on position i_min
    # and i is always higher than j in minimum distance calculation,
    # the root node must be at the last index
    return Tree(nodes[len(nodes)-1])